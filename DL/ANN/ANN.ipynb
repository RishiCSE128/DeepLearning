{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN Theory\n",
    "## Introduction\n",
    "* Deep learning was invented back in 1970 and got momentum at 2080 but soon after lost its pace as the technology back then was not edequate to compute those models\n",
    "* Another problem was to accommodae the huge amount of dataset, which is a critical component for any ML/DL model to get trained. Storage was costlier back then compared to present days\n",
    "* With the advacement of computer sciece,bothe these problem has been mitigated \n",
    "    - Storate prices are coming down rapidly with a logarithmic rate over the years, per-bit cost is almost halving every year [1] and scientists are now working genetic memories such as DNA. The major advantage being first the density ($10^{18} bytes/mm^{3}$), 6 times that of magnetic storage and secondly, longivity, DNA can retain data for centuries or even millenia if kept protected from humidity and light (e.g. Fossils). The last 3 decade have evident a steep hike in encoding and decoding capacity of genetic storage, from order of $10$ at 1988 to order of $10^9$  at 2019[2].\n",
    "    - Computers are becoming even faster \n",
    "* __Neuron__ : Nurve cell, the building blocks of human breain [3].\n",
    "    - Axon : the input carrier (receiver)\n",
    "    - Dendrites : the output propagater (transmitter) \n",
    "    - Synopsis : the joints where neurons meet (connection) \n",
    "* __Aritificial Neuron__ : Creating a mathematical model that mimincs the behaviour of bilogial neuron (Perceptron)\n",
    "* __Deep Neural Network__ : Creating a multiple hidden layers (multi-layer perceptron)\n",
    "\n",
    "## The Neuron\n",
    "The study of deep learning is to create biolofical neurons artificially and compute tasks through it which are considered hard in traditional computing models ( _soft-computing vs hard-computing_ ). Neuron was first depicted at 1895 [4]. The Axon and Dentrons actually don't make physical contact at the synaps, due to which a capacitence is formed which results a weight factor, termed as __Synaptic weight__.  \n",
    "![](synapse.jpg)\n",
    "A mathematical model of neuron was first devised by McCulloch and Pitts (McCulloch-Pitts model)[5] as a summing-amplifier. Neurons are modeled as a node, with some incidnet input connections ($X={x_i}$) with their respective (synaptic) weights ($w_i$). The input may come from sensors or some other neurons, Within the node, weighed inputs are summed and given to an activation function $\\phi\\bigg(\\sum_i{w_ix_i}\\bigg)$ which is propaged to the output $y$. \n",
    "\n",
    "The input layer comprises of a vector of independent variables, which denotes a single observation of the dataser or sample of the space or a touple from the database. the input vector must be standardised ($\\mu=0, \\sigma=1$) or normalised (z-score $\\in [o,1]$) to eliminate any bias[7]. The weights play a vital role in learning, they are responsible for prioritising input signals' straingths i.e. their contributiion to the output. During the learning phase the weights get adjusted, if $w_{i,j}$ be the weight if the synapse connecting neuron {i} and {j}, the learning process yeilds the calculation of $W=[w_{i,j}]$ metrix from an initial $W$, that can predict/classifly efficiently.  \n",
    "\n",
    "The output value can be regressive, i.e. single or categorical, i.e. multiple. Each case may have forms continious discrete or binary. \n",
    "\n",
    "### Activation functions\n",
    "Among various activation functions used in deep leaning, following are the most common ones.\n",
    "1. __Threshold Function__: $\\phi(x)=1 (x \\ge 0), 0$ otherwise \n",
    "2. __Sigmoid Function__: $\\phi(x)=\\frac{1}{1+e^{-x}}$, continius and bounded by $[0,1]$, typically used at the last layer as it's very useful to calculate probability. \n",
    "3. __Rectifier Function__:$\\phi(x)=max(0,x)$, the usage of this function can be found here [7]\n",
    "4. __Hyperbolic-tangent Function__: $\\phi(x)=\\frac{1-e^{-2x}}{1+e^{-2x}}$, liniar and bunded by $[-1,1]$\n",
    "\n",
    "## ANN working principle \n",
    "### The learning process\n",
    "ANNs are soft-computing models, the program is not hard-coded with decission rather the program must \"Learn\" appropriate decisions specific to the data-set. The Learnign process is given below. \n",
    "1. Load the first sample (row)\n",
    "    1. Step 1.1: supply actual output, $y$ and corresponding inputs, $X=\\{x_i\\}$, the ANN will predict an output, $\\hat{y}$.\n",
    "    2. step 1.2: calculate the deviation between actual and predicted outputs called __Cost__ $C=\\frac{(\\hat{y}-y)^2}{2}$\n",
    "    3. step 1.3: The value of the cost function is fed back to the network (__Back-Propagation__) to re-adjust the weights. The update is modeled as an optimization problem and solved by __Gradient Descent Algorithm__. \n",
    "    4. Repeat Step 2,3 untill the cost function is minimum.\n",
    "2. Load the next row, untill last row is reached \n",
    "3. End of one epoch, goto step 1, untill complete all pre-defined epochs.\n",
    "\n",
    "### The runtime process\n",
    "One the neural network is trained, the weigts are adjusted. the steps are given below,\n",
    "* Step 1 : get input from input layers i.e. $X$, weigh it i.e. $W^T.X$ and relay the activated value $\\phi\\bigg(W^T.X \\bigg)$. Number of neurons in input layer would be same as the number of attributes in the given dataset.\n",
    "* Step 2 : relayed value is propagted to next hidden layers as their inputs (goto step 1). the process continiues untill an output layer is reached.  \n",
    "* Core Facts\n",
    "    * Every neuron in  the hiddel layer tries to work out the relationshio of any possible combination of the preceeding neuron to the successor neuron. Thus more neurons in a given layer may accommodate more combinations.\n",
    "    * The wights connecting two neurons represents the contribution of the incident neuron to the specific calculation the subjected neuron is performing. \n",
    "    * More hidden layers accommodates inter-dependencies and degree of non-linearity\n",
    "    \n",
    "## The Gradient Descent Algoritm \n",
    "Finding the optimal weights to minimise the cost function is an NP-Hard problem, i.e. a bruteforce approach would take exponential time if the number of neuron increases, called __The curse of dimentionality__. E.g. With 5-5-1 configuration on input, hidden and output network, there are 25 weights, and a dataset of 1000 entries would cause the system calculate $1000^25 = 10^75$ combination. The fastest super-computer +++++++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "1. https://mkomo.com/cost-per-gigabyte\n",
    "2. https://www.nature.com/articles/s41576-019-0125-3.pdf?origin=ppub\n",
    "3. https://www.austincc.edu/histologyhelp/tissues/tx_nerv_tis.html\n",
    "4. https://en.wikipedia.org/wiki/Neuron\n",
    "5. https://link.springer.com/article/10.1007/BF02478259\n",
    "6. http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf\n",
    "7. http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
