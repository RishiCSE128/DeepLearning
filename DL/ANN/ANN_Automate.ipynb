{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks (ANN) \n",
    "## Introduction\n",
    "* Deep learning was invented back in 1970 and got momentum at 2080 but soon after lost its pace as the technology back then was not edequate to compute those models\n",
    "* Another problem was to accommodae the huge amount of dataset, which is a critical component for any ML/DL model to get trained. Storage was costlier back then compared to present days\n",
    "* With the advacement of computer sciece,bothe these problem has been mitigated \n",
    "    - Storate prices are coming down rapidly with a logarithmic rate over the years, per-bit cost is almost halving every year [1] and scientists are now working genetic memories such as DNA. The major advantage being first the density ($10^{18} bytes/mm^{3}$), 6 times that of magnetic storage and secondly, longivity, DNA can retain data for centuries or even millenia if kept protected from humidity and light (e.g. Fossils). The last 3 decade have evident a steep hike in encoding and decoding capacity of genetic storage, from order of $10$ at 1988 to order of $10^9$  at 2019[2].\n",
    "    - Computers are becoming even faster \n",
    "* __Neuron__ : Nurve cell, the building blocks of human breain [3].\n",
    "    - Axon : the input carrier (receiver)\n",
    "    - Dendrites : the output propagater (transmitter) \n",
    "    - Synopsis : the joints where neurons meet (connection) \n",
    "* __Aritificial Neuron__ : Creating a mathematical model that mimincs the behaviour of bilogial neuron (Perceptron)\n",
    "* __Deep Neural Network__ : Creating a multiple hidden layers (multi-layer perceptron)\n",
    "\n",
    "## The Neuron\n",
    "The study of deep learning is to create biolofical neurons artificially and compute tasks through it which are considered hard in traditional computing models ( _soft-computing vs hard-computing_ ). Neuron was first depicted at 1895 [4]. The Axon and Dentrons actually don't make physical contact at the synaps, due to which a capacitence is formed which results a weight factor, termed as __Synaptic weight__.  \n",
    "![](synapse.jpg)\n",
    "A mathematical model of neuron was first devised by McCulloch and Pitts (McCulloch-Pitts model)[5] as a summing-amplifier. Neurons are modeled as a node, with some incidnet input connections ($X={x_i}$) with their respective (synaptic) weights ($w_i$). The input may come from sensors or some other neurons, Within the node, weighed inputs are summed and given to an activation function $\\phi\\bigg(\\sum_i{w_ix_i}\\bigg)$ which is propaged to the output $y$. \n",
    "\n",
    "The input layer comprises of a vector of independent variables, which denotes a single observation of the dataser or sample of the space or a touple from the database. the input vector must be standardised ($\\mu=0, \\sigma=1$) or normalised (z-score $\\in [o,1]$) to eliminate any bias[7]. The weights play a vital role in learning, they are responsible for prioritising input signals' straingths i.e. their contributiion to the output. During the learning phase the weights get adjusted, if $w_{i,j}$ be the weight if the synapse connecting neuron {i} and {j}, the learning process yeilds the calculation of $W=[w_{i,j}]$ metrix from an initial $W$, that can predict/classifly efficiently.  \n",
    "\n",
    "The output value can be regressive, i.e. single or categorical, i.e. multiple. Each case may have forms continious discrete or binary. \n",
    "\n",
    "### Activation functions\n",
    "Among various activation functions used in deep leaning, following are the most common ones.\n",
    "1. __Threshold Function__: $\\phi(x)=1 (x \\ge 0), 0$ otherwise \n",
    "2. __Sigmoid Function__: $\\phi(x)=\\frac{1}{1+e^{-x}}$, continius and bounded by $[0,1]$, typically used at the last layer as it's very useful to calculate probability. \n",
    "3. __Rectifier Function__:$\\phi(x)=max(0,x)$, the usage of this function can be found here [7]\n",
    "4. __Hyperbolic-tangent Function__: $\\phi(x)=\\frac{1-e^{-2x}}{1+e^{-2x}}$, liniar and bunded by $[-1,1]$\n",
    "\n",
    "## ANN working principle \n",
    "### The learning process\n",
    "ANNs are soft-computing models, the program is not hard-coded with decission rather the program must \"Learn\" appropriate decisions specific to the data-set. The Learnign process is given below. \n",
    "1. Load the first sample (row)\n",
    "    1. Step 1.1: supply actual output, $y$ and corresponding inputs, $X=\\{x_i\\}$, the ANN will predict an output, $\\hat{y}$.\n",
    "    2. step 1.2: calculate the deviation between actual and predicted outputs called __Cost__ $C=\\frac{(\\hat{y}-y)^2}{2}$\n",
    "    3. step 1.3: The value of the cost function is fed back to the network (__Back-Propagation__) to re-adjust the weights. The update is modeled as an optimization problem and solved by __Gradient Descent Algorithm__. \n",
    "    4. Repeat Step 2,3 untill the cost function is minimum.\n",
    "2. Load the next row, untill last row is reached \n",
    "3. End of one epoch, goto step 1, untill complete all pre-defined epochs.\n",
    "\n",
    "### The runtime process\n",
    "One the neural network is trained, the weigts are adjusted. the steps are given below,\n",
    "* Step 1 : get input from input layers i.e. $X$, weigh it i.e. $W^T.X$ and relay the activated value $\\phi\\bigg(W^T.X \\bigg)$. Number of neurons in input layer would be same as the number of attributes in the given dataset.\n",
    "* Step 2 : relayed value is propagted to next hidden layers as their inputs (goto step 1). the process continiues untill an output layer is reached.  \n",
    "* Core Facts\n",
    "    * Every neuron in  the hiddel layer tries to work out the relationshio of any possible combination of the preceeding neuron to the successor neuron. Thus more neurons in a given layer may accommodate more combinations.\n",
    "    * The wights connecting two neurons represents the contribution of the incident neuron to the specific calculation the subjected neuron is performing. \n",
    "    * More hidden layers accommodates inter-dependencies and degree of non-linearity\n",
    "    \n",
    "## The Gradient Descent Algoritm \n",
    "Finding the optimal weights to minimise the cost function is an NP-Hard problem, i.e. a bruteforce approach would take exponential time if the number of neuron increases, called __The curse of dimentionality__. E.g. With 5-5-1 configuration on input, hidden and output network, there are 25 weights, and a dataset of 1000 entries would cause the system calculate $1000^{25} = 10^{75}$ combination. The fastest super-computer +++++++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "1. https://mkomo.com/cost-per-gigabyte\n",
    "2. https://www.nature.com/articles/s41576-019-0125-3.pdf?origin=ppub\n",
    "3. https://www.austincc.edu/histologyhelp/tissues/tx_nerv_tis.html\n",
    "4. https://en.wikipedia.org/wiki/Neuron\n",
    "5. https://link.springer.com/article/10.1007/BF02478259\n",
    "6. http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf\n",
    "7. http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an ANN\n",
    "Course URL : https://www.superdatascience.com/pages/deep-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Use-Case \n",
    "The problem is a classification one, the task is to predict based on the record which customer is likely to leave.  \n",
    "\n",
    "the subjected bank. The steps are as follows. \n",
    "1. __Import the required Library__ : __Keras__ is a opensource DL library that runs on top of __Theano__ or __Tensorflow__. It is prefered to start writing codes in Keras than directly on tensorflow because of its ease of building a Deep learning model. For ML enginner than DL developers or researcher, Keras is recomanded. The back-end engine i.e. Tensorflow or Theano is capable of running both on CPU and GPU\n",
    "2. __Data pre-processing__ : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk \n",
    "import keras as kr      # the frontend of DNN \n",
    "import tensorflow as tf # the backend of DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a sample of the data source "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_root = pd.read_csv('DS/Churn_Modelling.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n",
       "0          1    15634602  Hargrave          619    France  Female   42   \n",
       "1          2    15647311      Hill          608     Spain  Female   41   \n",
       "2          3    15619304      Onio          502    France  Female   42   \n",
       "3          4    15701354      Boni          699    France  Female   39   \n",
       "4          5    15737888  Mitchell          850     Spain  Female   43   \n",
       "\n",
       "   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "0       2       0.00              1          1               1   \n",
       "1       1   83807.86              1          0               1   \n",
       "2       8  159660.80              3          1               0   \n",
       "3       1       0.00              2          0               0   \n",
       "4       2  125510.82              1          1               1   \n",
       "\n",
       "   EstimatedSalary  Exited  \n",
       "0        101348.88       1  \n",
       "1        112542.58       0  \n",
       "2        113931.57       1  \n",
       "3         93826.63       0  \n",
       "4         79084.10       0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_root.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 14)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_root.shape   # dimension of the data set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 : Data pre-processing\n",
    "This section is explained in the Data-prepricessing chapter thus, the code is given only...\n",
    "\n",
    "Now, all these independent variables are not useful to build model, those ones must be removed for restricting the diention of the data matrix to relevent attributes only. \n",
    "1. customer id  - has no significance on the leaving \n",
    "2. surname      - not significant \n",
    "3. credit score - is significant \n",
    "4. geography    - is significant \n",
    "5. Gender       - not sure thus taking in considersation \n",
    "6. Age          - is significant, younger people are more agile than stable \n",
    "7. Tenure       - is significant, as it says how long one is a customer \n",
    "8. Balance      - is significant \n",
    "9. Number prod  - may be, it says how many banking services the customer enjoyes\n",
    "10. Credit card - is siginificant\n",
    "11. active member - yes \n",
    "12. Salary      - yes \n",
    "\n",
    "therefore the useful indices are = [3,4,5,6,7,8,9,10,11,12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [RowNumber, CustomerId, Surname, CreditScore, Geography, Gender, Age, Tenure, Balance, NumOfProducts, HasCrCard, IsActiveMember, EstimatedSalary, Exited]\n",
       "Index: []"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_root[ds_root.isna().any(axis=1)]  # to see which column has missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds_root.copy()\n",
    "# Dependent-Independent variable split\n",
    "X_idx = [3,4,5,6,7,8,9,10,11,12] # list of valid independent variables\n",
    "X = ds.iloc[ : , X_idx].values\n",
    "y = ds.iloc[ : , -1].values\n",
    "\n",
    "# Removing any missing data\n",
    "from sklearn.impute import SimpleImputer\n",
    "col_na = [] \n",
    "imputer = SimpleImputer(missing_values=np.nan , strategy='mean')\n",
    "#X[: , col_na] = imputer.fit_transform(X[: , col_na])  #since there's no missing data \n",
    "\n",
    "# encoding categorical variables\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "le = LabelEncoder()\n",
    "X[:,1] = le.fit_transform(X[:,1])\n",
    "X[:,2] = le.fit_transform(X[:,2])\n",
    "\n",
    "col_tans = make_column_transformer( \n",
    "                        (sk.preprocessing.OneHotEncoder(), \n",
    "                        [1]), \n",
    "                        remainder='passthrough')\n",
    "X = col_tans.fit_transform(X)\n",
    "\n",
    "X = X[: , 1:]     #eleminate col 0 to remove dummy variable trap \n",
    "\n",
    "# Train Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3 , random_state = 0)\n",
    "\n",
    "# Feature Scalling \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scale_X = StandardScaler()\n",
    "X_train[: , 5:13] = scale_X.fit_transform(X_train[: , 5:13])\n",
    "X_test[: , 5:13] = scale_X.transform(X_test[: , 5:13])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>641</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>-1.03635</td>\n",
       "      <td>1.13249</td>\n",
       "      <td>0.810394</td>\n",
       "      <td>0.641985</td>\n",
       "      <td>0.966835</td>\n",
       "      <td>-0.768624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>541</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>0.697009</td>\n",
       "      <td>-1.19975</td>\n",
       "      <td>0.810394</td>\n",
       "      <td>0.641985</td>\n",
       "      <td>-1.0343</td>\n",
       "      <td>-1.3936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>590</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "      <td>0.00366482</td>\n",
       "      <td>1.36838</td>\n",
       "      <td>-0.929716</td>\n",
       "      <td>-1.55767</td>\n",
       "      <td>0.966835</td>\n",
       "      <td>-1.49739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>516</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00366482</td>\n",
       "      <td>-1.19975</td>\n",
       "      <td>-0.929716</td>\n",
       "      <td>-1.55767</td>\n",
       "      <td>0.966835</td>\n",
       "      <td>0.801015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>508</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>0.697009</td>\n",
       "      <td>1.08573</td>\n",
       "      <td>-0.929716</td>\n",
       "      <td>0.641985</td>\n",
       "      <td>0.966835</td>\n",
       "      <td>0.512914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6995</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>594</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>-0.343007</td>\n",
       "      <td>0.71582</td>\n",
       "      <td>0.810394</td>\n",
       "      <td>0.641985</td>\n",
       "      <td>0.966835</td>\n",
       "      <td>1.09316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6996</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>794</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>-0.343007</td>\n",
       "      <td>0.625928</td>\n",
       "      <td>-0.929716</td>\n",
       "      <td>0.641985</td>\n",
       "      <td>0.966835</td>\n",
       "      <td>0.134014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6997</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>738</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>0.00366482</td>\n",
       "      <td>1.37308</td>\n",
       "      <td>0.810394</td>\n",
       "      <td>0.641985</td>\n",
       "      <td>-1.0343</td>\n",
       "      <td>1.414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6998</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>590</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>1.39035</td>\n",
       "      <td>-1.19975</td>\n",
       "      <td>0.810394</td>\n",
       "      <td>0.641985</td>\n",
       "      <td>0.966835</td>\n",
       "      <td>0.846258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6999</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>623</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>-1.38302</td>\n",
       "      <td>0.524404</td>\n",
       "      <td>-0.929716</td>\n",
       "      <td>0.641985</td>\n",
       "      <td>-1.0343</td>\n",
       "      <td>0.326893</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7000 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0  1    2  3   4           5         6         7         8         9  \\\n",
       "0     0  0  641  1  33    -1.03635   1.13249  0.810394  0.641985  0.966835   \n",
       "1     0  0  541  1  39    0.697009  -1.19975  0.810394  0.641985   -1.0343   \n",
       "2     0  0  590  0  76  0.00366482   1.36838 -0.929716  -1.55767  0.966835   \n",
       "3     0  1  516  1  50  0.00366482  -1.19975 -0.929716  -1.55767  0.966835   \n",
       "4     0  0  508  0  60    0.697009   1.08573 -0.929716  0.641985  0.966835   \n",
       "...  .. ..  ... ..  ..         ...       ...       ...       ...       ...   \n",
       "6995  1  0  594  0  32   -0.343007   0.71582  0.810394  0.641985  0.966835   \n",
       "6996  0  1  794  0  22   -0.343007  0.625928 -0.929716  0.641985  0.966835   \n",
       "6997  0  0  738  1  35  0.00366482   1.37308  0.810394  0.641985   -1.0343   \n",
       "6998  0  1  590  0  38     1.39035  -1.19975  0.810394  0.641985  0.966835   \n",
       "6999  1  0  623  0  48    -1.38302  0.524404 -0.929716  0.641985   -1.0343   \n",
       "\n",
       "            10  \n",
       "0    -0.768624  \n",
       "1      -1.3936  \n",
       "2     -1.49739  \n",
       "3     0.801015  \n",
       "4     0.512914  \n",
       "...        ...  \n",
       "6995   1.09316  \n",
       "6996  0.134014  \n",
       "6997     1.414  \n",
       "6998  0.846258  \n",
       "6999  0.326893  \n",
       "\n",
       "[7000 rows x 11 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 1, 1, 0])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 : Build an ANN \n",
    "In this step we will build the ANN using __`keras`__ library with tensorflow backend. first we need to modules \n",
    "1. __`keras.models.Sequental`__ that initializes the ANN, and let you add layers one-by-one \n",
    "2. __`keras.layers.Dense`__ that creates a Dense layer to the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras as kr \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all the libraries are imported, we need to create an ANN by adding Layers\n",
    "1. Initialize the ANN object using the __`Sequential`__ class \n",
    "2. Add layer using the __`add()`__ function, with number of nodes equals to number of independent variables\n",
    "3. Keep adding hidden layers, the number of nodes is subjected to experimentation and evaluation thus there is no rule-of-thumb. The art of experimenting with the ANN architecture to obtain the optimal form is called __Hyper-parameter Tunig__, this includes methods suchs as _K-fold Cross-validation_ etc. (Tip : _however, a common tip for experiece be, the number of nodes in the hidden layer is the average of the number of nodes in input layer and output layers $|L_h| = \\frac{|L_i|+|L_o|}{2}$_ )  \n",
    "    1. specify input neuron\n",
    "    2. number of neuron at the given hidden layer \n",
    "    3. initial randomiser for the synaptic weights \n",
    "    4. activation funtion, we're using __ReLu__ for hidden layers and __Sigmoid__ for output \n",
    "4. Add an output layer by setting the __`output_dim=n`__ where.\n",
    "    1. `n=1` for Binary classification or regression \n",
    "    2. `n=k` for a k-class classification. \n",
    "    3. the `activation` parameter is __`Sigmoid`__ for `n=1` and __`softmax`__ for `n=k`\n",
    "5. You can noe verify the random assignment of the synaptic weights using the __`<ANN_object>.weight`__ command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rishi/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(input_dim=11, activation=\"relu\", units=6, kernel_initializer=\"uniform\")`\n",
      "  \"\"\"\n",
      "/home/rishi/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=6, kernel_initializer=\"uniform\")`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/home/rishi/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\")`\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "ANN_classifier = Sequential() # init a ANN object\n",
    "ANN_classifier.add(Dense(input_dim = 11 ,      # No. of input neuron (only for 1st layer)\n",
    "                         output_dim = 6,       # No. of neuron on this layer = (11+1)/2\n",
    "                         init = 'uniform' ,    # initial randomiser = uniform dist\n",
    "                         activation = 'relu' ) # activation func\n",
    "                  )\n",
    "\n",
    "ANN_classifier.add(Dense(output_dim = 6,       # Adding another hidden layer\n",
    "                         init = 'uniform',\n",
    "                         activation = 'relu')\n",
    "                  )\n",
    "\n",
    "ANN_classifier.add(Dense(output_dim = 1,       # Dep. variable is binary \n",
    "                         init = 'uniform',\n",
    "                         activation = 'sigmoid')\n",
    "                  ) # sigmoid fn yeilds probability \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 : Compile the model\n",
    "The __`compile()`__ function build the ANN datastucture, this includes several hyper-parameters\n",
    "1. __Optimizer__ : name of the optimization algorithm used in learning process with SGDA\n",
    "2. __Loss__ : loss function calculates the differennce between actual and predicted value.\n",
    "    1. if the dependent variable is binary-class, the loss function is __`binary_crossentopy`__\n",
    "    2. if the dependent variable is multi-class, the loss function is __`categorical_crossentropy`__\n",
    "3. __metric__ : a list of metrics that evaluates each iteration of the training process and determinses when to stop. There are several metric parameters such as,\n",
    "    1. Accuracy \n",
    "    2. loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANN_classifier.compile(optimizer='adam',\n",
    "                       loss='binary_crossentropy',\n",
    "                       metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 : Train the ANN\n",
    "The __`fit`__ function feeds the training sets to train the ANN. Hyper-parameter needed are,\n",
    "1. __batch_size__ : number of observation ofter which the weights are to be updated \n",
    "2. __epochs__ : number of rounds of repeatation \n",
    "\n",
    "There is no rule-of-thumb in orer to choose these values, by experiment the following values are obtained, here we use the default values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken... 31.654 seconds\n",
      "Accuracy... 0.781000018119812 -->                     0.7979999780654907\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t = time.time()\n",
    "hist_1 = ANN_classifier.fit(X_train, y_train, \n",
    "                   batch_size=32,\n",
    "                   epochs=50, \n",
    "                   verbose=False)\n",
    "print(f'time taken... {round(time.time() - t , 3)} seconds')\n",
    "print(f'Accuracy... {round( hist_1.history[\"accuracy\"][0] ,3)} --> \\\n",
    "                    {round( hist_1.history[\"accuracy\"][-1] ,3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training process can be made faster using __`Call_back`__ feature \n",
    "* Callbacks are functions that can be applied at certain stages of the training process, such as at the end of each epoch. Specifically, in our solution, we included __`EarlyStopping(monitor='val_loss', patience=2)`__ to define that we wanted to monitor the test (validation) loss at each epoch and after the test loss has not improved after two epochs, training is interrupted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken... 1.327 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from keras.callbacks import EarlyStopping\n",
    "t = time.time()\n",
    "hist_2 = ANN_classifier.fit(X_train, y_train, \n",
    "                   batch_size=32,\n",
    "                   epochs=50, \n",
    "                   callbacks=[EarlyStopping(monitor='loss', patience=1)],\n",
    "                   verbose=False)\n",
    "print(f'time taken... {round(time.time() - t , 3)} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "plt.plot(hist_1.history['loss'],'ro:',label='w/o ES');\n",
    "plt.plot(hist_2.history['loss'],'g*:',label ='with ES');\n",
    "plt.plot([0,len(hist_1.history['loss'])],\n",
    "         [hist_1.history['loss'][-1],hist_1.history['loss'][-1]])\n",
    "plt.plot([0,len(hist_1.history['loss'])],\n",
    "         [hist_2.history['loss'][-1],hist_2.history['loss'][-1]])\n",
    "plt.legend()\n",
    "plt.xlabel='Iteration'\n",
    "plt.ylabel='Loss'\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = ANN_classifier.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2379,    0],\n",
       "       [ 621,    0]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an ANN Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras as ka\n",
    "import tensorflow as tf \n",
    "import sklearn as sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-24-8ebaa18a8293>, line 37)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-24-8ebaa18a8293>\"\u001b[0;36m, line \u001b[0;32m37\u001b[0m\n\u001b[0;31m    if len(le_cols) != 0\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def pre_processing(data_source, \n",
    "                   nan_cols, \n",
    "                   le_cols, \n",
    "                   ohe_cols, \n",
    "                   is_dmy_trp, \n",
    "                   is_cat_y, \n",
    "                   test_size, \n",
    "                   is_scale_y):\n",
    "    '''\n",
    "    Performs data pre-processing for ML/DL programs \n",
    "    data_source: a STRING to locate data source CSV file\n",
    "    nan_cols   : a LIST of colums that has NaN entries (use 'ds[ds.isna().any(axis=1)]' to locate)  \n",
    "    le_cols    : a LIST of colums which to be label encoded\n",
    "    ohe_cols   : a LIST of column which to be One-hot encoded\n",
    "    is_dmy_trap: a BOOLEAN value, if set to true one dummy col will be remove to resolve dummy_var_trap\n",
    "    is_cat_y   : a BOOLEAN value, True if dependent variable is categorical\n",
    "    test_size  : a NUMBER in [0,1], proportion of the Test size for splitting \n",
    "    is_scale_y : a BOOLEAN value, True if dependent variable is to scale (recomanded for Regression)\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    # loading data source\n",
    "    ds = pd.read_csv(data_source)   \n",
    "    \n",
    "    # splitting dependent and independent variables\n",
    "    X = ds.iloc[ : , :-1].values\n",
    "    y = ds.iloc[ : , -1].values\n",
    "    \n",
    "    # removing NaN values\n",
    "    if len(nan_cols) != 0 :\n",
    "        from sklearn.impute import SimpleImputer\n",
    "        imputer = SimpleImputer(missing_values=np.nan , \n",
    "                                strategy='mean')\n",
    "        X[: , nan_cols ] = imputer.fit_transform(X[ : , nan_cols])\n",
    "\n",
    "    # Encoding categorical Variables\n",
    "    if len(le_cols) != 0\n",
    "        from sklearn.preprocessing import LabelEncoder\n",
    "        le_X = LabelEncoder()\n",
    "        for col in le_cols:\n",
    "            X[ : , col ] = le_X.fit_transform(X[ : , col])\n",
    "    \n",
    "    if is_cat_y:\n",
    "        le_y = LabelEncoder()\n",
    "        y = le_y.fit_transform(y)\n",
    "        \n",
    "    if len(ohe_cols) != 0:\n",
    "        from sklearn.compose import make_column_transformer\n",
    "        col_trans = make_column_transformer( (OneHotEncoder(), \n",
    "                                              ohe_cols),\n",
    "                                              remainder='passthrough'\n",
    "                                           )\n",
    "        X=col_trans.fit_transform(X)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if is_dmy_trp:\n",
    "            X = X[ : , 1:]\n",
    "    \n",
    "    # Train-test split\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                        test_size = test_size, \n",
    "                                                        random_state = 0)\n",
    "    # Feature Scalling\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaller_X = StandardScaler()\n",
    "    X_train = scaller_X.fit_transform(X_train)\n",
    "    X_test = scaller_X.transform(X_test)\n",
    "    \n",
    "    if is_scale_y:\n",
    "        scaller_y = StandardScaler()\n",
    "        y_train = scaller_y.fit_transform(y_train)\n",
    "        y_test = scaller_y.transform(y_test)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (<ipython-input-25-7cbccfbbddc2>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-25-7cbccfbbddc2>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    a,b,c,d = pre_processing(data_source= 'DS/Churn_Modelling.csv',nan_cols=[],cat\u001b[0m\n\u001b[0m                                                                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "a,b,c,d = pre_processing(data_source= 'DS/Churn_Modelling.csv',nan_cols=[],cat\n",
    "             \n",
    "                        \n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ann_classifier(X_train, y_train, dnn_arch):\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense\n",
    "    from keras.callbacks import EarlyStopping\n",
    "    import time\n",
    "    \n",
    "    classifier = Sequential()\n",
    "    \n",
    "    for n in range(len(dnn_arch['layers'])):\n",
    "        print('Adding layer : ' , dnn_arch['layers'][n])\n",
    "        if n==0:\n",
    "            classifier.add(Dense(input_dim = X_train.shape[1], \n",
    "                                 output_dim = dnn_arch['layers'][n]['nodes'],\n",
    "                                 init = 'uniform',\n",
    "                                 activation = dnn_arch['layers'][n]['act'])\n",
    "                          )\n",
    "        else:\n",
    "             classifier.add(Dense(output_dim = dnn_arch['layers'][n]['nodes'],\n",
    "                                 init = 'uniform',\n",
    "                                 activation = dnn_arch['layers'][n]['act'])\n",
    "                          )\n",
    "                \n",
    "    classifier.compile(optimizer=dnn_arch['opt'], \n",
    "                       loss=dnn_arch['loss'],\n",
    "                       metrics=['accuracy'])\n",
    "    \n",
    "    print('Training Initialised...')\n",
    "    \n",
    "    t=time.time()\n",
    "    \n",
    "    his = classifier.fit(X_train, y_train,\n",
    "                   batch_size = dnn_arch['batch'],\n",
    "                   epochs = dnn_arch['epoch'],\n",
    "                   callbacks = [EarlyStopping(monitor='loss', patience=2)],\n",
    "                   verbose = False)\n",
    "    \n",
    "    \n",
    "    duration = time.time() - t\n",
    "    start_loss = his.history['loss'][0]\n",
    "    final_loss = his.history['loss'][-1]\n",
    "    \n",
    "    print(f'Traing complete in {round(duration,3)} seconds with loss value : {start_loss} ---> {final_loss}')\n",
    "    \n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_arch={\n",
    "    'layers' : [{'nodes' : 6 , 'act' : 'relu' },\n",
    "                {'nodes' : 6 , 'act' : 'relu' },\n",
    "                {'nodes' : 1 , 'act' : 'sigmoid'}],\n",
    "    'opt' : 'adam',\n",
    "    'loss': 'binary_crossentropy',\n",
    "    'batch': 32,\n",
    "    'epoch':100\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'layers': [{'nodes': 6, 'act': 'relu'},\n",
       "  {'nodes': 6, 'act': 'relu'},\n",
       "  {'nodes': 1, 'act': 'sigmoid'}],\n",
       " 'opt': 'adam',\n",
       " 'loss': 'binary_crossentropy',\n",
       " 'batch': 32,\n",
       " 'epoch': 100}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn_arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding layer :  {'nodes': 6, 'act': 'relu'}\n",
      "Adding layer :  {'nodes': 6, 'act': 'relu'}\n",
      "Adding layer :  {'nodes': 1, 'act': 'sigmoid'}\n",
      "Training Initialised...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rishi/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(input_dim=11, activation=\"relu\", units=6, kernel_initializer=\"uniform\")`\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/rishi/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:20: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=6, kernel_initializer=\"uniform\")`\n",
      "/home/rishi/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:20: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traing complete in 17.561 seconds with loss value : 0.5360353295008341 ---> 0.4483371512889862\n"
     ]
    }
   ],
   "source": [
    "model = build_ann_classifier(X_train, y_train, dnn_arch=dnn_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
