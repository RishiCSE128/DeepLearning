{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Self Organising Maps (SOMs) were invented in 1980 by Tuevo Kohonen[1],is an unsupervised learning algorithm mainly used for Dimentionality reduction[2]. It takes a multi-dimentional dataset and map it to a 2D representation. Unlike a Supervised lerning, SOM learns to group data from a training set[3]. \n",
    "![](som.png)\n",
    "\n",
    "## K-means Quick recap\n",
    "K-means clustering is relevent to SOM but not exactly identical, however it helps to understand SOMs. K-means is a clustering algorithm, thus it helps to group a dataset.Steps are as follows \n",
    "1. Choose the number of cluster $k$\n",
    "2. Select a random $k$ points, the centroids (not nessesesarily from the dataset) \n",
    "3. Assign each data point to the closest centroid which eventually formes $k$ clusters\n",
    "4. compute and place the new centroid of each cluster \n",
    "5. Reassign each data point to the new closest centrioid. \n",
    "    1. If any reassignment took place goto Step 4, \n",
    "    2. otherwise finish \n",
    "![](KMeans.png)\n",
    "\n",
    "### Random Initialization trap  in K-Means \n",
    "A random initialization of initial centroid would cause a miss-clustering. Thus __k-Means++__ algorithm solves the problem \n",
    "![](kmeans2.png)\n",
    "\n",
    "### Choosing right number of clusters \n",
    "\n",
    "\n",
    "\n",
    "## SOM Learning Mechanism \n",
    "Let a Detaset with $n$ number of samples with attributes (dimensions) and they are mapped into a $9$ nodes map. SOM makes a dense neural net that produces all possible combinations of $3$ features into $9$ possibilities. Each possibility is evaluated with euclidian distence $d_j = \\sqrt{\\sum{(x_i - w_{j,i})^2}} | i\\in\\{1...3\\}, j\\in\\{1...9\\}$ the output node with $min(d_j)$ is called the __BMU__ or the best matching unit. \n",
    "![](som2.png)\n",
    "this actually brings the map closer to the data point as it chooses the map with minimum distance, hence the map eventually self organises itself to the dataset. Once the BMU is decided, a set of maps (weights corresponding output nodes) within a redius $r$ centered at BMU gets updated, so then they also get closer to the data-point. The amount of update takes place is inversly proportional to the disttence to the BMU. Each BMU will then start competing with eachother by pulling other nodes based on the distance distance with them. \n",
    "![](som3.png)\n",
    "Once the BMUs are updated, the learning mechanism in each epoch reduces hence the influence srinks. \n",
    "![](som4.png)\n",
    "\n",
    "## SOM features\n",
    "1. SOM retains the topology of the input set \n",
    "2. SOM reveals correlation that are not easily identified \n",
    "3. SOM classifies data without supervision hence no need of target vector and hence no back-propagation \n",
    "4. No lateral connection between output nodes \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "1. https://ieeexplore.ieee.org/document/58325\n",
    "2. https://arxiv.org/pdf/1312.5753.pdf\n",
    "3. http://www.ai-junkie.com/ann/som/som1.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
